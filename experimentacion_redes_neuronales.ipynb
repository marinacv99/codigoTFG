{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" Cargamos las librerías necesarias para el desarrollo de redes neuronales:","metadata":{}},{"cell_type":"code","source":"import sys\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nfrom keras.applications import Xception, InceptionResNetV2\nfrom keras.layers import Input, Dense, Flatten, Conv2D,Conv1D, MaxPooling2D, Dropout, AveragePooling2D,MaxPooling1D,AveragePooling1D,BatchNormalization\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nimport zipfile\nimport shutil\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nimport librosa\nimport librosa.display\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-11-25T13:07:54.271309Z","iopub.execute_input":"2021-11-25T13:07:54.271871Z","iopub.status.idle":"2021-11-25T13:08:03.39302Z","shell.execute_reply.started":"2021-11-25T13:07:54.271775Z","shell.execute_reply":"2021-11-25T13:08:03.391035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comprobamos que se está utilizando la GPU para reducir el tiempo de inferencia de las redes neuronales","metadata":{}},{"cell_type":"code","source":"device_type = \"GPU\"\n\ndef check_physical_devices(device_type):\n    \"\"\"Check that a device type is used in the host runtime.\"\"\"\n    physical_devices = tf.config.list_physical_devices(device_type)\n\n    if not physical_devices:\n        raise RuntimeError(f\"No {device_type} devices are used in the host.\")","metadata":{"execution":{"iopub.status.busy":"2021-10-26T07:53:39.020486Z","iopub.execute_input":"2021-10-26T07:53:39.020923Z","iopub.status.idle":"2021-10-26T07:53:39.028433Z","shell.execute_reply.started":"2021-10-26T07:53:39.020883Z","shell.execute_reply":"2021-10-26T07:53:39.027005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Detección de emociones mediante imágenes","metadata":{}},{"cell_type":"markdown","source":"## Carga de datos\n","metadata":{}},{"cell_type":"markdown","source":"Dividimos el conjunto de datos en entrenamiento,validación y prueba usando un muestreo estratificado","metadata":{}},{"cell_type":"code","source":"seed = 27965 # establish a seed to make the experiments reproducible as Keras doesn't asure that reproduciblity\n#set the test and validation ratio\ntest_ratio = 0.2\nvalidation_ratio = 0.2\n\n# Seed is established\nnp.random.seed(seed)\n\n# Directories are created, if they already exists then they are deleted previously\nif os.path.isdir(\"validation\"):\n    shutil.rmtree(\"validation\")\nif os.path.isdir(\"train\"):\n    shutil.rmtree(\"train\")\nif os.path.isdir(\"test\"):\n    shutil.rmtree(\"test\")\n\nos.mkdir(\"validation\")\nos.mkdir(\"train\")\nos.mkdir(\"test\")\n\nemotions = os.listdir(\"../input/fer2013/train\") #store the name of the emotions in a list\n\n#remove 2 emotions to make more easy the training for the neural network\nemotions.remove(\"surprise\")\nemotions.remove(\"disgust\")\n\ntotal_dataset_images = 0\n#for each emotion create a folder\nfor emotion in emotions:    \n    print(emotion)\n    path = \"../input/fer2013/test/\" + emotion\n    path2 = \"../input/fer2013/train/\" + emotion\n    total_dataset_images += len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n    total_dataset_images += len([f for f in os.listdir(path2) if os.path.isfile(os.path.join(path2, f))])\n    os.mkdir(\"validation/\" + str(emotion))\n    os.mkdir(\"train/\" + str(emotion))\n    os.mkdir(\"test/\" + str(emotion))\n   \nprint(\"Total amount of images in the dataset: \" + str(total_dataset_images))\n\n#calculate the percentage and the number of images per class to achieve the stratified sampling\npercentages_per_class_dict = {}\nfor emotion in emotions:    \n    print(emotion)\n    path = \"../input/fer2013/test/\" + emotion\n    path2 = \"../input/fer2013/train/\" + emotion\n    percentages_per_class_dict[emotion] = (len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]) + len([f for f in os.listdir(path2) if os.path.isfile(os.path.join(path2, f))]))/total_dataset_images\n\nimages_per_class_dict = {}\nfor emotion in emotions:    \n    path = \"../input/fer2013/test/\" + emotion\n    path2 = \"../input/fer2013/train/\" + emotion\n    images_of_emotion = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))] + [f for f in os.listdir(path2) if os.path.isfile(os.path.join(path2, f))]\n    np.random.shuffle(images_of_emotion)\n    images_per_class_dict[emotion] = images_of_emotion\n\nimages_for_test = int(total_dataset_images * test_ratio)\nimages_for_train_total = total_dataset_images - images_for_test\nimages_for_validation = int(images_for_train_total * validation_ratio)\nimages_for_train = images_for_train_total - images_for_validation\n   \nimages_per_class_test_proportional_dict = {}\nfor emotion in emotions:    \n    images_per_class_test_proportional_dict[emotion] = int(images_for_test*percentages_per_class_dict[emotion])\n   \nimages_per_class_train_proportional_dict = {}\nfor emotion in emotions:    \n    images_per_class_train_proportional_dict[emotion] = int(images_for_train*percentages_per_class_dict[emotion])\n   \nimages_per_class_validation_proportional_dict = {}\nfor emotion in emotions:    \n    images_per_class_validation_proportional_dict[emotion] = int(images_for_validation*percentages_per_class_dict[emotion])\n   \n#Test images are copied to the test folder\nfor emotion in emotions:\n    print(emotion)\n    path = \"../input/fer2013/test/\" + emotion\n    path2 = \"../input/fer2013/train/\" + emotion\n   \n    images_amount = images_per_class_test_proportional_dict[emotion]\n    emotion_images = images_per_class_dict.get(emotion)\n    while images_amount > 0:\n        image = emotion_images.pop(0)\n        images_per_class_dict[emotion] = emotion_images\n       \n        if os.path.isfile(os.path.join(path, image)):\n            shutil.copy(path + \"/\" + image, \"./test/\" + emotion)\n        else:\n            shutil.copy(path2 + \"/\" + image, \"./test/\" + emotion)\n       \n        images_amount -= 1\n       \nprint(\"Images moved to test folder\")\n       \n# Train images are copied to the train folder\nfor emotion in emotions:\n    path = \"../input/fer2013/test/\" + emotion\n    path2 = \"../input/fer2013/train/\" + emotion\n   \n    images_amount = images_per_class_train_proportional_dict[emotion]\n    emotion_images = images_per_class_dict.get(emotion)\n    while images_amount > 0:\n        image = emotion_images.pop(0)\n        images_per_class_dict[emotion] = emotion_images\n       \n        if os.path.isfile(os.path.join(path, image)):\n            shutil.copy(path + \"/\" + image, \"./train/\" + emotion)\n        else:\n            shutil.copy(path2 + \"/\" + image, \"./train/\" + emotion)\n       \n        images_amount -= 1\n       \nprint(\"Images moved to train folder\")\n       \n# Validation images are copied to the train folder\nfor emotion in emotions:\n    path = \"../input/fer2013/test/\" + emotion\n    path2 = \"../input/fer2013/train/\" + emotion\n   \n    images_amount = images_per_class_validation_proportional_dict[emotion]\n    emotion_images = images_per_class_dict.get(emotion)\n    while images_amount > 0:\n        image = emotion_images.pop(0)\n        images_per_class_dict[emotion] = emotion_images\n       \n        if os.path.isfile(os.path.join(path, image)):\n            shutil.copy(path + \"/\" + image, \"./validation/\" + emotion)\n        else:\n            shutil.copy(path2 + \"/\" + image, \"./validation/\" + emotion)\n           \n        images_amount -= 1\n\nprint(\"Images moved to validation folder\")","metadata":{"execution":{"iopub.status.busy":"2021-10-26T07:53:39.031418Z","iopub.execute_input":"2021-10-26T07:53:39.031809Z","iopub.status.idle":"2021-10-26T07:56:11.095038Z","shell.execute_reply.started":"2021-10-26T07:53:39.031743Z","shell.execute_reply":"2021-10-26T07:56:11.09409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definimos el generador de imágenes para el training, validation y test, mediante ImagenDataGenerator.\n\nEn el training aplicaremos data augmentation para evitar el sobreajuste ya que así añadimos algo de ruido a las imágenes:\n\n* Normalizamos los valores de los píxeles entre 0 y 1\n* Aplicamos un 0.2 de zoom\n* Añadimos un desplazamiento de la imagen tanto vertical como horizontal de 0.2\n* Finalmente establecemos un rango para el brillo de la imagen de entre [0.1,0.7]\n","metadata":{}},{"cell_type":"code","source":"\ntraining_imagen_generator = ImageDataGenerator(rescale=1/255,zoom_range=0.2,brightness_range=[0.1,0.7], width_shift_range=0.2,\n    height_shift_range=0.2,)\nvalidation_imagen_generator = ImageDataGenerator(rescale=1/ 255)\ntest_imagen_generator = ImageDataGenerator(rescale=1/ 255)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T07:56:11.096879Z","iopub.execute_input":"2021-10-26T07:56:11.097219Z","iopub.status.idle":"2021-10-26T07:56:11.105162Z","shell.execute_reply.started":"2021-10-26T07:56:11.097188Z","shell.execute_reply":"2021-10-26T07:56:11.103832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahora generamos los iteradores para cada generador creado previamente. \n\nDefinimos un valor de batch de 64, lo que significa que se propagarán 64 imágenes en paralelo a través de la red neuronal, y establecemos un tamaño de imagen final.\n","metadata":{}},{"cell_type":"code","source":"directory=\"train\"\nbatch= 64\ntarget_size = (48,48)\ntraining_iterator = training_imagen_generator.flow_from_directory(seed=seed,\n                                                     directory=directory,\n                                                     batch_size=batch,\n                                                    target_size=target_size, color_mode = \"grayscale\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-26T07:56:11.108342Z","iopub.execute_input":"2021-10-26T07:56:11.108742Z","iopub.status.idle":"2021-10-26T07:56:11.976075Z","shell.execute_reply.started":"2021-10-26T07:56:11.108709Z","shell.execute_reply":"2021-10-26T07:56:11.9749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory=\"validation\"\nvalidation_iterator = validation_imagen_generator.flow_from_directory(seed=seed,\n                                                     directory=directory,\n                                                     batch_size=batch,\n                                                     target_size=target_size,color_mode = \"grayscale\")","metadata":{"execution":{"iopub.status.busy":"2021-10-26T07:56:11.97814Z","iopub.execute_input":"2021-10-26T07:56:11.978651Z","iopub.status.idle":"2021-10-26T07:56:12.199129Z","shell.execute_reply.started":"2021-10-26T07:56:11.978563Z","shell.execute_reply":"2021-10-26T07:56:12.197526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory=\"test\"\ntest_iterator = test_imagen_generator.flow_from_directory(seed=seed,\n                                                     directory=directory,\n                                                     batch_size=batch,\n                                                     target_size=target_size,color_mode = \"grayscale\")","metadata":{"execution":{"iopub.status.busy":"2021-10-26T07:56:12.201027Z","iopub.execute_input":"2021-10-26T07:56:12.201361Z","iopub.status.idle":"2021-10-26T07:56:12.522241Z","shell.execute_reply.started":"2021-10-26T07:56:12.201332Z","shell.execute_reply":"2021-10-26T07:56:12.521066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Arquitectura de la red neuronal","metadata":{}},{"cell_type":"markdown","source":"A continuación se han ido creando las diversas arquitecturas de redes convolucionales para hacer pruebas","metadata":{}},{"cell_type":"code","source":"activation = \"relu\"\ninput_shape=(*target_size,1)\n\nconvolutional_layer_1 = Conv2D(32, 12, activation=activation, input_shape=input_shape)\nconvolutional_layer_2 = Conv2D(64, 9, activation=activation, padding='same' )\nconvolutional_layer_3 = Conv2D(128,3, activation=activation, padding='same')\nconvolutional_layer_4 = Conv2D(128, 3, activation=activation, padding='same')\nconvolutional_layer_5 = Conv2D(128, 3, activation=activation, padding='same')\nconvolutional_layer_6 = Conv2D(128, 3, activation=activation, padding='same')\n\navg_pooling_layer_1 = AveragePooling2D(3)\navg_pooling_layer_2 = AveragePooling2D(2)\navg_pooling_layer_3 = AveragePooling2D(2)\n\nmax_pooling_layer_1 = MaxPooling2D(3)\nmax_pooling_layer_2 = MaxPooling2D(2)\nmax_pooling_layer_3 = MaxPooling2D(2)\nmax_pooling_layer_4 = MaxPooling2D(2)\n\nflatten_layer = Flatten(data_format=\"channels_last\")\n\ndropout_layer_1 = Dropout(0.25, seed=seed)\ndropout_layer_2 = Dropout(0.25, seed=seed)\ndropout_layer_3 = Dropout(0.25, seed=seed)\n\nhidden_layer_1 = Dense(1024, activation=activation)\nhidden_layer_2 = Dense(512, activation=activation)\nhidden_layer_3 = Dense(512, activation=activation)\n\nactivation = \"softmax\"\noutput_layer = Dense(3, activation='softmax')\n\n\nmodel = Sequential()\n\nmodel.add(convolutional_layer_1)\nmodel.add(max_pooling_layer_1)\n\nmodel.add(convolutional_layer_2)\nmodel.add(convolutional_layer_3)\nmodel.add(max_pooling_layer_2)\nmodel.add(dropout_layer_1)\nmodel.add(convolutional_layer_4)\nmodel.add(max_pooling_layer_3)\nmodel.add(convolutional_layer_5)\nmodel.add(max_pooling_layer_4)\nmodel.add(dropout_layer_2)\n\nmodel.add(flatten_layer)\n\nmodel.add(dropout_layer_3)\n\n\nmodel.add(hidden_layer_2)\n\nmodel.add(output_layer)\n\n\nlr = 1e-3\noptimizer = Adam(lr=lr)\nmetrics = \"accuracy\"\nloss = \"categorical_crossentropy\"\n\n#show the model\nfrom keras.utils.vis_utils import plot_model\n# model.summary()\nplot_model(model, to_file='model_plot_50.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T15:22:32.042877Z","iopub.execute_input":"2021-10-04T15:22:32.043217Z","iopub.status.idle":"2021-10-04T15:22:32.365117Z","shell.execute_reply.started":"2021-10-04T15:22:32.043178Z","shell.execute_reply":"2021-10-04T15:22:32.363839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compilamos el modelo creado y comienza el entrenamiento de la red. Para ello, se selecciona el conjunto de entrenamiento, y se realiza una validación con el conjunto de validación.","metadata":{}},{"cell_type":"code","source":"model.compile(loss=loss,\n              metrics=metrics,\n                  optimizer=optimizer)\ncheckpoint = ModelCheckpoint(\"best.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\nepochs = 50\ngenerator = training_iterator \nvalidation_data = validation_iterator \nprobabilities = model.fit_generator(epochs=epochs,generator=generator, validation_data = validation_data, callbacks=[checkpoint])\nmodel.save(\"best_image_model_3_2.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-10-04T15:22:50.300669Z","iopub.execute_input":"2021-10-04T15:22:50.301031Z","iopub.status.idle":"2021-10-04T15:29:13.485624Z","shell.execute_reply.started":"2021-10-04T15:22:50.300997Z","shell.execute_reply":"2021-10-04T15:29:13.48396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahora procedemos a la predicción utilizando el conjunto de prueba, una vez la red ha sido entrenada","metadata":{}},{"cell_type":"code","source":"generator = test_iterator\nconvolutional_neural_network_predictions = model.predict(generator)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T12:40:51.979663Z","iopub.execute_input":"2021-09-24T12:40:51.980078Z","iopub.status.idle":"2021-09-24T12:40:52.012184Z","shell.execute_reply.started":"2021-09-24T12:40:51.980044Z","shell.execute_reply":"2021-09-24T12:40:52.010477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mostramos los resultados en una matriz de confusión","metadata":{}},{"cell_type":"code","source":"\ny_pred = [np.argmax(probas) for probas in  convolutional_neural_network_predictions]\ny_test = test_iterator.classes\nclass_names = test_iterator.class_indices.keys()\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \n# compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T11:08:45.86506Z","iopub.execute_input":"2021-09-06T11:08:45.865419Z","iopub.status.idle":"2021-09-06T11:08:46.270208Z","shell.execute_reply.started":"2021-09-06T11:08:45.865389Z","shell.execute_reply":"2021-09-06T11:08:46.269077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ensemble de imágenes","metadata":{}},{"cell_type":"markdown","source":"Creamos un ensemble para utilizar varios modelos con la intención de mejorar los resultados obtenidos.","metadata":{}},{"cell_type":"code","source":"def load_models(models):\n    loaded_models = []\n    for i,model in enumerate(models):\n        mod = load_model(model)\n        mod._name = f\"model_{i}\" \n        loaded_models.append(mod)\n    return loaded_models\n\n\n\ndef ensemble(models,input_layer,final_layers):\n    models = load_models(models)\n    output_layer = [model(input_layer) for model in models]\n    if isinstance(final_layers,list):\n        for layer in final_layers:\n            output_layer = layer(output_layer)\n    else:\n        output_layer = final_layers(output_layer)\n    return Model(input_layer, output_layer)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T15:13:34.061326Z","iopub.execute_input":"2021-11-23T15:13:34.061696Z","iopub.status.idle":"2021-11-23T15:13:34.070081Z","shell.execute_reply.started":"2021-11-23T15:13:34.061667Z","shell.execute_reply":"2021-11-23T15:13:34.068619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\nfrom keras.layers import Average\n\n\n#models with 5 emotions\nmodel1_5 = \"../input/image-models/image_5_emotions_new.h5\"\nmodel2_5 = \"../input/image-models/image_5_emotions_new_2.h5\"\nmodel3_5 = \"../input/image-models/image_5_emotions_new_3.h5\"\n\nmodels=[model1_5,model2_5, model3_5]\ninput_layer = Input(shape=(48,48,1))\noutput_layer = Average()\n\nensemble =ensemble(models,input_layer,output_layer)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T15:15:53.869189Z","iopub.execute_input":"2021-11-23T15:15:53.869554Z","iopub.status.idle":"2021-11-23T15:15:53.913504Z","shell.execute_reply.started":"2021-11-23T15:15:53.869521Z","shell.execute_reply":"2021-11-23T15:15:53.911288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\nensemble.save('ensemble_image_5_emotions\".h5')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T14:04:21.982354Z","iopub.execute_input":"2021-11-23T14:04:21.98289Z","iopub.status.idle":"2021-11-23T14:04:22.091496Z","shell.execute_reply.started":"2021-11-23T14:04:21.982852Z","shell.execute_reply":"2021-11-23T14:04:22.090279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluamos el ensemble","metadata":{}},{"cell_type":"code","source":"ensemble.evaluate(test_iterator)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T08:56:19.135735Z","iopub.execute_input":"2021-10-26T08:56:19.136307Z","iopub.status.idle":"2021-10-26T08:56:24.463918Z","shell.execute_reply.started":"2021-10-26T08:56:19.136258Z","shell.execute_reply":"2021-10-26T08:56:24.462076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble_predictions = ensemble.predict(test_iterator)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T11:19:09.539543Z","iopub.execute_input":"2021-10-05T11:19:09.539924Z","iopub.status.idle":"2021-10-05T11:20:09.534369Z","shell.execute_reply.started":"2021-10-05T11:19:09.53989Z","shell.execute_reply":"2021-10-05T11:20:09.533544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ny_pred = [np.argmax(probas) for probas in  ensemble_predictions]\ny_test = test_iterator.classes\nclass_names = test_iterator.class_indices.keys()\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \n# compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-05T11:51:04.737751Z","iopub.execute_input":"2021-10-05T11:51:04.738168Z","iopub.status.idle":"2021-10-05T11:51:05.062286Z","shell.execute_reply.started":"2021-10-05T11:51:04.738131Z","shell.execute_reply":"2021-10-05T11:51:05.061078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred\ndf['Actual Labels'] = y_test\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T11:51:31.431739Z","iopub.execute_input":"2021-10-05T11:51:31.43215Z","iopub.status.idle":"2021-10-05T11:51:31.453745Z","shell.execute_reply.started":"2021-10-05T11:51:31.432099Z","shell.execute_reply":"2021-10-05T11:51:31.453009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Detección mediante audio","metadata":{}},{"cell_type":"markdown","source":"Creamos un dataframe con la ruta de cada archivo WAV junto con la emoción que expresa cada uno, para poder trabajar con los datos","metadata":{}},{"cell_type":"code","source":"data = \"../input/cremad/AudioWAV/\"\ncrema_directory_list = os.listdir(data)\n\nfile_emotion = []\nfile_path = []\n\nangry = 0\nfear = 0\ndis = 0\nneu = 0\n\nfor file in crema_directory_list:   \n    part=file.split('_')\n    if part[2] == 'SAD':\n        file_path.append(data + file)\n        file_emotion.append('sad')\n    elif part[2] == 'ANG':\n        angry += 1\n        file_path.append(data + file)\n        file_emotion.append('angry')\n    elif part[2] == 'HAP':\n        file_path.append(data + file)\n        file_emotion.append('happy')\n    elif part[2] == 'NEU':\n        file_path.append(data + file)\n        file_emotion.append('neutral')\n        dis+=1\n    elif part[2] == 'FEA':\n        file_path.append(data + file)\n        file_emotion.append('fear')\n        fear+=1\n    \n    \nprint(\"angry : \"+ str(angry))\nprint(\"disg: \" +str(dis))\nprint(\"feaar: \"+ str(fear))       \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nCrema_df = pd.concat([emotion_df, path_df], axis=1)\nCrema_df.Path\nCrema_df.Emotions\nCrema_df.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T13:08:03.395239Z","iopub.execute_input":"2021-11-25T13:08:03.395769Z","iopub.status.idle":"2021-11-25T13:08:04.137708Z","shell.execute_reply.started":"2021-11-25T13:08:03.395722Z","shell.execute_reply":"2021-11-25T13:08:04.136313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtenemos conjuntos de train y test","metadata":{}},{"cell_type":"code","source":"X = Crema_df.Path\nY = Crema_df.Emotions\n\n\n# Discretizamos la variable y.\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n\n\n# dividimos en test y train\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=1, train_size = 0.8, test_size = 0.2, shuffle=True)\nx_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test, train_size = 0.5, test_size=0.5, random_state=1, shuffle=True)\n\n\nx_train.shape, y_train.shape, x_test.shape, y_test.shape, x_valid.shape, y_valid.shape\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T13:08:05.283154Z","iopub.execute_input":"2021-11-25T13:08:05.283546Z","iopub.status.idle":"2021-11-25T13:08:05.300363Z","shell.execute_reply.started":"2021-11-25T13:08:05.28351Z","shell.execute_reply":"2021-11-25T13:08:05.298982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Realizamos data augmentation al conjunto de entrenamiento para evitar sobreajuste. \nAñadimos a los audios ruido, cambiamos la velocidad, el tono de la voz y el tiempo","metadata":{}},{"cell_type":"code","source":"def noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\n\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n\n#cogemos el primer elemento del conjunto de entrenamiento como ejemplo\npath = np.array(x_train)[0]\ndata, sample_rate = librosa.load(path)\nsample_rate","metadata":{"execution":{"iopub.status.busy":"2021-11-25T13:08:08.141673Z","iopub.execute_input":"2021-11-25T13:08:08.142277Z","iopub.status.idle":"2021-11-25T13:08:08.914081Z","shell.execute_reply.started":"2021-11-25T13:08:08.142133Z","shell.execute_reply":"2021-11-25T13:08:08.912966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Representamos un ejemplo","metadata":{}},{"cell_type":"code","source":"def create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n    librosa.display.waveplot(data, sr=sr)\n    plt.show()\n\ndef create_spectrogram(data, sr, e):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')   \n    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:49:29.108466Z","iopub.execute_input":"2021-10-11T09:49:29.109053Z","iopub.status.idle":"2021-10-11T09:49:29.116794Z","shell.execute_reply.started":"2021-10-11T09:49:29.108974Z","shell.execute_reply":"2021-10-11T09:49:29.116006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=data, sr=sample_rate)\nAudio(path)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-08T17:47:01.766727Z","iopub.execute_input":"2021-10-08T17:47:01.767133Z","iopub.status.idle":"2021-10-08T17:47:01.986375Z","shell.execute_reply.started":"2021-10-08T17:47:01.767092Z","shell.execute_reply":"2021-10-08T17:47:01.985038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = noise(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:08:01.533205Z","iopub.execute_input":"2021-08-17T11:08:01.533559Z","iopub.status.idle":"2021-08-17T11:08:01.692494Z","shell.execute_reply.started":"2021-08-17T11:08:01.533529Z","shell.execute_reply":"2021-08-17T11:08:01.691707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = stretch(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:57:36.236399Z","iopub.execute_input":"2021-08-17T14:57:36.236755Z","iopub.status.idle":"2021-08-17T14:57:36.464139Z","shell.execute_reply.started":"2021-08-17T14:57:36.236699Z","shell.execute_reply":"2021-08-17T14:57:36.463174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = pitch(data, sample_rate)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T23:36:27.063423Z","iopub.execute_input":"2021-06-11T23:36:27.064002Z","iopub.status.idle":"2021-06-11T23:36:27.309803Z","shell.execute_reply.started":"2021-06-11T23:36:27.063958Z","shell.execute_reply":"2021-06-11T23:36:27.308856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seleccionamos las siguientes transformaciones para obtener características y transformar el audio en un formato válido, usando sample_rate y sample_data :\n","metadata":{}},{"cell_type":"code","source":"def extract_features(data):\n    # ZCR: tasa de cambio de signo de la señal\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    result=np.hstack((result, zcr)) # stacking horizontally\n    \n\n#     Chroma_stft: a partir de una onda calcula un cromagrama\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, chroma_stft)) # stacking horizontally\n    \n\n    # MFCC: representan el habla en función a la percepción humana\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mfcc)) # stacking horizontally\n    \n\n    # Root Mean Square Value: amplitud media cuadrada en un intervalo de tiempo\n    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n    result = np.hstack((result, rms)) # stacking horizontally\n    \n\n    # MelSpectogram : espectograma\n    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mel)) # stacking horizontally\n\n    \n    return result\n\ndef get_features(path):\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    \n    # without augmentation\n    res1 = extract_features(data)\n    result = np.array(res1)\n    # data with noise\n    noise_data = noise(data)\n    res2 = extract_features(noise_data)\n    result = np.vstack((result, res2)) # stacking vertically\n    # data with stretching and pitching\n    new_data = stretch(data)\n    data_stretch_pitch = pitch(new_data, sample_rate)\n    res3 = extract_features(data_stretch_pitch)\n    result = np.vstack((result, res3)) # stacking vertically\n    return np.array(result)\n\ndef get_features_no_augmentation(path):\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    \n    # without augmentation\n    res1 = extract_features(data)\n    result = np.array(res1)\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2021-11-25T13:08:11.986466Z","iopub.execute_input":"2021-11-25T13:08:11.986789Z","iopub.status.idle":"2021-11-25T13:08:11.99758Z","shell.execute_reply.started":"2021-11-25T13:08:11.98676Z","shell.execute_reply":"2021-11-25T13:08:11.9967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aplicamos las características con el data augmentation al conjunto de entrenamiento","metadata":{}},{"cell_type":"code","source":"X1, Y1 = [], []\n\n# for path, emotion  in training():\ncont = 0\nfor path, emotion in zip(x_train, y_train): \n    feature = get_features(path)\n    for ele in feature:\n        cont+=1\n        X1.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y1.append(emotion)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T13:08:15.606088Z","iopub.execute_input":"2021-11-25T13:08:15.606586Z","iopub.status.idle":"2021-11-25T13:42:11.94351Z","shell.execute_reply.started":"2021-11-25T13:08:15.606552Z","shell.execute_reply":"2021-11-25T13:42:11.942151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aplicamos las características al conjunto de test","metadata":{}},{"cell_type":"code","source":"X2, Y2 = [], []\n\n# for path, emotion  in test():\nfor path, emotion in zip(x_test, y_test):\n    feature = get_features_no_augmentation(path) \n    X2.append(feature)\n    Y2.append(emotion)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T13:42:11.945721Z","iopub.execute_input":"2021-11-25T13:42:11.946524Z","iopub.status.idle":"2021-11-25T13:43:41.685329Z","shell.execute_reply.started":"2021-11-25T13:42:11.946469Z","shell.execute_reply":"2021-11-25T13:43:41.684133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aplicamos las características al conjunto de validación\n","metadata":{}},{"cell_type":"code","source":"X3, Y3 = [], []\n\n# for path, emotion  in validation():\nfor path, emotion in zip(x_valid, y_valid):\n    feature = get_features_no_augmentation(path) \n    X3.append(feature)\n    Y3.append(emotion)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T13:43:41.691881Z","iopub.execute_input":"2021-11-25T13:43:41.694902Z","iopub.status.idle":"2021-11-25T13:45:10.224473Z","shell.execute_reply.started":"2021-11-25T13:43:41.694789Z","shell.execute_reply":"2021-11-25T13:45:10.223296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## entrenamiento de la red","metadata":{}},{"cell_type":"markdown","source":"escalamos y preparamos los datos para hacerlos compatibles para la red","metadata":{}},{"cell_type":"code","source":"y_train =np.array(Y1)\ny_test = np.array(Y2)\nx_train = np.array(X1)\nx_test  =  np.array(X2)\nx_validation = np.array(X3)\ny_validation = np.array(Y3)\n\nscaler = StandardScaler()\nx_train_model = scaler.fit_transform(x_train)\nx_valid_model = scaler.fit_transform(x_validation)\nx_test_model = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-25T13:45:10.226117Z","iopub.execute_input":"2021-11-25T13:45:10.226813Z","iopub.status.idle":"2021-11-25T13:45:10.366153Z","shell.execute_reply.started":"2021-11-25T13:45:10.226769Z","shell.execute_reply":"2021-11-25T13:45:10.365127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = np.expand_dims(x_train_model, axis=2)\nx_validation = np.expand_dims(x_valid_model, axis=2)\nx_test = np.expand_dims(x_test_model, axis=2)\nx_train.shape, y_train.shape, x_validation.shape, y_validation.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-25T13:45:10.367571Z","iopub.execute_input":"2021-11-25T13:45:10.368331Z","iopub.status.idle":"2021-11-25T13:45:10.376945Z","shell.execute_reply.started":"2021-11-25T13:45:10.368282Z","shell.execute_reply":"2021-11-25T13:45:10.376081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creamos las diversas arquitecturas de red para hacer pruebas","metadata":{}},{"cell_type":"code","source":"from keras.layers import LSTM,BatchNormalization, Activation, Bidirectional, GRU,AveragePooling1D, MaxPooling1D\n\nmodel = Sequential()\n\nmodel.add(Conv1D(64, kernel_size=9, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(Conv1D(128, kernel_size=9, strides=1, padding='same', activation='relu'))\nmodel.add(AveragePooling1D(pool_size=2, strides = 2, padding = 'same'))\n\n\nmodel.add(Conv1D(128, kernel_size=9, strides=1, padding='same', activation='relu'))\nmodel.add(AveragePooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv1D(128, kernel_size=9, strides=1, padding='same', activation='relu'))\nmodel.add(Conv1D(256, kernel_size=6, strides=1, padding='same', activation='relu'))\nmodel.add(AveragePooling1D(pool_size=3, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu'))\nmodel.add(Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu'))\nmodel.add(AveragePooling1D(pool_size=3, strides = 2, padding = 'same'))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv1D(256, kernel_size=3, strides=2, padding='same', activation='relu'))\nmodel.add(Conv1D(256, kernel_size=3, strides=2, padding='same', activation='relu'))\nmodel.add(AveragePooling1D(pool_size=3, strides = 2, padding = 'same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=512, activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(units=128, activation='relu')) \n\n\nmodel.add(Dense(5, activation=\"softmax\"))\nmodel.summary()\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy',  metrics = ['accuracy'])\n\nhist = model.fit(x_train, y_train, validation_data= (x_validation, y_validation), epochs = 30, batch_size = 32)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T10:02:40.090837Z","iopub.execute_input":"2021-10-28T10:02:40.091332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Guardamos y evaluamos los resultados con el conjunto de test","metadata":{}},{"cell_type":"code","source":"model.save('audio_6_model_5_emotions_new.h5')","metadata":{"execution":{"iopub.status.busy":"2021-10-28T09:58:17.33236Z","iopub.execute_input":"2021-10-28T09:58:17.332751Z","iopub.status.idle":"2021-10-28T09:58:17.427874Z","shell.execute_reply.started":"2021-10-28T09:58:17.332715Z","shell.execute_reply":"2021-10-28T09:58:17.426923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(x_test,y_test)[1]*100","metadata":{"execution":{"iopub.status.busy":"2021-10-28T09:57:44.617696Z","iopub.execute_input":"2021-10-28T09:57:44.618039Z","iopub.status.idle":"2021-10-28T09:57:45.283289Z","shell.execute_reply.started":"2021-10-28T09:57:44.618011Z","shell.execute_reply":"2021-10-28T09:57:45.282223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predecimos\npred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-28T09:57:57.994527Z","iopub.execute_input":"2021-10-28T09:57:57.994875Z","iopub.status.idle":"2021-10-28T09:57:58.807185Z","shell.execute_reply.started":"2021-10-28T09:57:57.994847Z","shell.execute_reply":"2021-10-28T09:57:58.806363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score( y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T16:49:04.310068Z","iopub.execute_input":"2021-09-23T16:49:04.310538Z","iopub.status.idle":"2021-09-23T16:49:04.315959Z","shell.execute_reply.started":"2021-09-23T16:49:04.310507Z","shell.execute_reply":"2021-09-23T16:49:04.315286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mostramos los resultados","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T10:54:31.857148Z","iopub.execute_input":"2021-10-05T10:54:31.857501Z","iopub.status.idle":"2021-10-05T10:54:31.887012Z","shell.execute_reply.started":"2021-10-05T10:54:31.857472Z","shell.execute_reply":"2021-10-05T10:54:31.885871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T09:58:02.861831Z","iopub.execute_input":"2021-10-28T09:58:02.86246Z","iopub.status.idle":"2021-10-28T09:58:03.200313Z","shell.execute_reply.started":"2021-10-28T09:58:02.86242Z","shell.execute_reply":"2021-10-28T09:58:03.199569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"markdown","source":"## Ensemble de audio","metadata":{}},{"cell_type":"markdown","source":"Creamos un ensemble para unir varios modelos del audio con intención de mejorar los resultados obtenidos","metadata":{}},{"cell_type":"code","source":"def load_models(models):\n    loaded_models = []\n    for i,model in enumerate(models):\n        mod = load_model(model)\n        mod._name = f\"model_{i}\" \n        loaded_models.append(mod)\n    return loaded_models\n\ndef ensemble(models,input_layer,final_layers):\n    models = load_models(models)\n    output_layer = [model(input_layer) for model in models]\n    if isinstance(final_layers,list):\n        for layer in final_layers:\n            output_layer = layer(output_layer)\n    else:\n        output_layer = final_layers(output_layer)\n    return Model(input_layer, output_layer)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T11:35:23.041267Z","iopub.execute_input":"2021-11-23T11:35:23.041691Z","iopub.status.idle":"2021-11-23T11:35:23.050612Z","shell.execute_reply.started":"2021-11-23T11:35:23.041655Z","shell.execute_reply":"2021-11-23T11:35:23.049187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\nfrom keras.layers import Average\n\n#models with 5 emotions\nmodel1=\"../input/audio-models/audio_1_model_5_emotions.h5\"\nmodel2= \"../input/audio-models/audio_2_model_5_emotions.h5\"\nmodel3= \"../input/audio-models/audio_3_model_5_emotions.h5\"\nmodel4= \"../input/audio-models/audio_4_model_5_emotions.h5\"\nmodel5= \"../input/audio-models/audio_5_model_5_emotions.h5\"\nmodel6= \"../input/audio-models/audio_6_model_5_emotions.h5\"\n\nmodels =[model1, model2, model3, model4, model5, model6]\n\ninput_shape = (162, 1)\nmodel_input = Input(shape=input_shape)\noutput_layer= Average()\n\nensemble =ensemble(models,model_input,output_layer)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-23T11:35:29.228844Z","iopub.execute_input":"2021-11-23T11:35:29.229529Z","iopub.status.idle":"2021-11-23T11:35:37.274322Z","shell.execute_reply.started":"2021-11-23T11:35:29.229492Z","shell.execute_reply":"2021-11-23T11:35:37.273049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compilamos y evaluamos los resultados","metadata":{}},{"cell_type":"code","source":"ensemble.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\nensemble.save('ensemble_audio_5_emotions.h5')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T11:36:06.22007Z","iopub.execute_input":"2021-11-23T11:36:06.220782Z","iopub.status.idle":"2021-11-23T11:36:06.44465Z","shell.execute_reply.started":"2021-11-23T11:36:06.220698Z","shell.execute_reply":"2021-11-23T11:36:06.443349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.evaluate(x_test,y_test)[1]*100","metadata":{"execution":{"iopub.status.busy":"2021-10-26T09:00:45.228335Z","iopub.execute_input":"2021-10-26T09:00:45.228922Z","iopub.status.idle":"2021-10-26T09:00:52.927118Z","shell.execute_reply.started":"2021-10-26T09:00:45.22888Z","shell.execute_reply":"2021-10-26T09:00:52.92555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test = ensemble.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\ny_test = encoder.inverse_transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T09:01:38.611031Z","iopub.execute_input":"2021-10-26T09:01:38.611801Z","iopub.status.idle":"2021-10-26T09:01:40.228929Z","shell.execute_reply.started":"2021-10-26T09:01:38.611749Z","shell.execute_reply":"2021-10-26T09:01:40.227341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T09:01:42.750511Z","iopub.execute_input":"2021-10-26T09:01:42.751202Z","iopub.status.idle":"2021-10-26T09:01:42.759782Z","shell.execute_reply.started":"2021-10-26T09:01:42.751161Z","shell.execute_reply":"2021-10-26T09:01:42.758375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mostramos los resultados","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(15)","metadata":{"execution":{"iopub.status.busy":"2021-10-26T09:08:10.935554Z","iopub.execute_input":"2021-10-26T09:08:10.936051Z","iopub.status.idle":"2021-10-26T09:08:10.957981Z","shell.execute_reply.started":"2021-10-26T09:08:10.936012Z","shell.execute_reply":"2021-10-26T09:08:10.956739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-26T09:02:05.257034Z","iopub.execute_input":"2021-10-26T09:02:05.257474Z","iopub.status.idle":"2021-10-26T09:02:05.710372Z","shell.execute_reply.started":"2021-10-26T09:02:05.257436Z","shell.execute_reply":"2021-10-26T09:02:05.709054Z"},"trusted":true},"execution_count":null,"outputs":[]}]}